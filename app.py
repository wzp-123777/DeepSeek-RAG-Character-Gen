import streamlit as st
import os
import tempfile
import json
import re

# è®¾ç½® HuggingFace é•œåƒï¼Œè§£å†³å›½å†…è¿æ¥é—®é¢˜
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from llm_client import LLMClient
from rag_engine import RAGEngine
from history_utils import save_history_item, load_history, delete_history_item

RAG_CONFIG_FILE = "rag_config.json"

def save_rag_config(config):
    try:
        with open(RAG_CONFIG_FILE, "w", encoding="utf-8") as f:
            json.dump(config, f)
    except Exception as e:
        print(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")

def load_rag_config():
    if os.path.exists(RAG_CONFIG_FILE):
        try:
            with open(RAG_CONFIG_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            pass
    return None

# é¡µé¢é…ç½®
st.set_page_config(page_title="DeepSeek RAG è§’è‰²ç”Ÿæˆå™¨", layout="wide")

# åˆå§‹åŒ– Session State
if "messages" not in st.session_state:
    st.session_state.messages = []
if "gen_messages" not in st.session_state:
    st.session_state.gen_messages = []
if "rag_engine" not in st.session_state:
    st.session_state.rag_engine = None
if "llm_client" not in st.session_state:
    st.session_state.llm_client = None
if "vector_db_ready" not in st.session_state:
    st.session_state.vector_db_ready = False

def init_rag(embedding_type, model_name, api_key=None, base_url=None):
    try:
        return RAGEngine(
            embedding_type=embedding_type, 
            model_name=model_name,
            api_key=api_key,
            base_url=base_url
        )
    except Exception as e:
        st.error(f"åˆå§‹åŒ– RAG å¼•æ“å¤±è´¥: {e}")
        return None

# å°è¯•è‡ªåŠ¨åŠ è½½æœ¬åœ°çŸ¥è¯†åº“é…ç½®
if not st.session_state.rag_engine and os.path.exists("./chroma_db") and os.path.exists(RAG_CONFIG_FILE):
    config = load_rag_config()
    if config:
        e_type = config.get("embedding_type")
        api_key_to_use = config.get("api_key")
        
        # å¦‚æœæ˜¯ API æ¨¡å¼ä¸”æ²¡æœ‰ä¿å­˜ Keyï¼Œå°è¯•ä»ç”¨æˆ·é…ç½®è¯»å–ï¼ˆå‡è®¾å¤ç”¨ï¼‰
        if e_type == "api" and not api_key_to_use:
             # è¿™é‡Œéœ€è¦ä¸´æ—¶åŠ è½½ä¸€ä¸‹ user config
             if os.path.exists("user_config.json"):
                 try:
                     with open("user_config.json", "r", encoding="utf-8") as f:
                         user_conf = json.load(f)
                         api_key_to_use = user_conf.get("api_key")
                 except:
                     pass

        if e_type == "local" or (e_type == "api" and api_key_to_use):
            with st.spinner("æ­£åœ¨è‡ªåŠ¨åŠ è½½ä¸Šæ¬¡çš„çŸ¥è¯†åº“..."):
                st.session_state.rag_engine = init_rag(
                    embedding_type=e_type,
                    model_name=config["model_name"],
                    api_key=api_key_to_use,
                    base_url=config.get("base_url")
                )
                if st.session_state.rag_engine:
                    # ç®€å•çš„éªŒè¯ä¸€ä¸‹æ˜¯å¦çœŸçš„æœ‰æ•°æ®
                    # è¿™é‡Œä¸è¿›è¡Œæ·±å±‚æ£€æŸ¥ï¼Œå‡è®¾ chroma_db å­˜åœ¨å³æœ‰æ•ˆ
                    st.session_state.vector_db_ready = True
                    # st.toast("å·²è‡ªåŠ¨åŠ è½½ä¸Šæ¬¡çš„çŸ¥è¯†åº“") # toast åœ¨è¿™é‡Œå¯èƒ½æ˜¾ç¤ºä¸å‡ºæ¥ï¼Œå› ä¸ºè¿˜æ²¡æ¸²æŸ“é¡µé¢

USER_CONFIG_FILE = "user_config.json"

def save_user_config(config):
    try:
        with open(USER_CONFIG_FILE, "w", encoding="utf-8") as f:
            json.dump(config, f)
    except Exception as e:
        print(f"ä¿å­˜ç”¨æˆ·é…ç½®å¤±è´¥: {e}")

def load_user_config():
    if os.path.exists(USER_CONFIG_FILE):
        try:
            with open(USER_CONFIG_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            pass
    return {}

def main():
    st.title("ğŸ¤– DeepSeek RAG è§’è‰²æç¤ºè¯ç”Ÿæˆå™¨")
    
    # åŠ è½½ç”¨æˆ·é…ç½®
    user_config = load_user_config()
    
    # --- ä¾§è¾¹æ é…ç½® ---
    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")
        
        # API é…ç½®
        default_provider_index = 0
        if user_config.get("api_provider") == "siliconflow":
            default_provider_index = 1
            
        api_provider = st.selectbox("é€‰æ‹© LLM æä¾›å•†", ["deepseek", "siliconflow"], index=default_provider_index)
        
        default_api_key = user_config.get("api_key", "")
        api_key = st.text_input("API Key", value=default_api_key, type="password", help="è¾“å…¥å¯¹åº”çš„ API Key")
        
        # åˆå§‹åŒ– LLM Client
        if api_key:
            try:
                st.session_state.llm_client = LLMClient(provider=api_provider, api_key=api_key)
                models = st.session_state.llm_client.get_available_models()
                
                default_model_index = 0
                saved_model = user_config.get("model_name")
                if saved_model and saved_model in models:
                    default_model_index = models.index(saved_model)
                
                selected_model = st.selectbox("é€‰æ‹©å¯¹è¯æ¨¡å‹", models, index=default_model_index)
                st.success(f"å·²è¿æ¥åˆ° {api_provider}")
                
                # ä¿å­˜é…ç½®ï¼ˆå½“è¿æ¥æˆåŠŸæ—¶ï¼‰
                if api_key != user_config.get("api_key") or api_provider != user_config.get("api_provider") or selected_model != user_config.get("model_name"):
                    save_user_config({
                        "api_provider": api_provider,
                        "api_key": api_key,
                        "model_name": selected_model
                    })
                    
            except Exception as e:
                st.error(f"è¿æ¥å¤±è´¥: {e}")
        else:
            st.warning("è¯·è¾“å…¥ API Key ä»¥ç»§ç»­")

        st.divider()

        # RAG é…ç½®
        st.subheader("ğŸ“š çŸ¥è¯†åº“æ„å»º")
        
        rag_config = load_rag_config() or {}
        
        # é»˜è®¤é€‰ä¸­ä¸Šæ¬¡çš„æ¨¡å¼
        default_mode_index = 0
        if rag_config.get("embedding_type") == "api":
            default_mode_index = 1
            
        rag_mode = st.radio("Embedding æ¨¡å¼", ["æœ¬åœ° (HuggingFace)", "äº‘ç«¯ API (SiliconFlow)"], index=default_mode_index)
        
        embedding_model_name = ""
        rag_api_key = None
        rag_base_url = None
        
        if rag_mode == "æœ¬åœ° (HuggingFace)":
            default_model = rag_config.get("model_name", "sentence-transformers/all-MiniLM-L6-v2")
            if rag_config.get("embedding_type") != "local": # å¦‚æœä¸Šæ¬¡ä¸æ˜¯ localï¼Œå°±ç”¨é»˜è®¤å€¼
                 default_model = "sentence-transformers/all-MiniLM-L6-v2"
                 
            embedding_model_name = st.text_input("æ¨¡å‹åç§°", value=default_model)
            st.caption("æç¤ºï¼šé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ã€‚å·²é…ç½®å›½å†…é•œåƒåŠ é€Ÿã€‚")
        else:
            # å°è¯•æ‰¾åˆ°ä¸Šæ¬¡ä½¿ç”¨çš„æ¨¡å‹ index
            model_options = [
                    "BAAI/bge-m3", 
                    "BAAI/bge-large-zh-v1.5", 
                    "Qwen/Qwen3-Embedding-8B", # ç”¨æˆ·æŒ‡å®š
                    "netease-youdao/bce-embedding-base_v1"
                ]
            default_emb_index = 0
            if rag_config.get("embedding_type") == "api" and rag_config.get("model_name") in model_options:
                default_emb_index = model_options.index(rag_config.get("model_name"))
                
            embedding_model_name = st.selectbox(
                "é€‰æ‹© Embedding æ¨¡å‹", 
                model_options,
                index=default_emb_index
            )
            st.caption("æ¨èä½¿ç”¨ BAAI/bge-m3 æˆ– Qwen/Qwen3-Embedding-8B")
            # é»˜è®¤å¤ç”¨ä¸Šé¢çš„ API Keyï¼Œå¦‚æœç”¨æˆ·éœ€è¦å•ç‹¬è®¾ç½®ä¹Ÿå¯ä»¥
            use_same_key = st.checkbox("ä½¿ç”¨ä¸Šæ–¹ç›¸åŒçš„ API Key", value=True)
            if use_same_key:
                rag_api_key = api_key
            else:
                rag_api_key = st.text_input("Embedding API Key", type="password")
            
            rag_base_url = "https://api.siliconflow.cn/v1"

        uploaded_files = st.file_uploader("ä¸Šä¼ å¤§æ–‡æœ¬ (txt, pdf, docx)", accept_multiple_files=True)
        kb_name = st.text_input("ç›®æ ‡çŸ¥è¯†åº“åç§° (ä»…é™å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿)", value="default_kb", help="å°†æ–‡ä»¶å­˜å…¥æŒ‡å®šçš„çŸ¥è¯†åº“åˆ†ç»„ä¸­ã€‚æ³¨æ„ï¼šä¸æ”¯æŒä¸­æ–‡ï¼Œé•¿åº¦3-63å­—ç¬¦ã€‚")
        
        if st.button("æ„å»º/æ›´æ–° çŸ¥è¯†åº“"):
            # æ ¡éªŒçŸ¥è¯†åº“åç§°
            if not re.match(r'^[a-zA-Z0-9][a-zA-Z0-9._-]{1,61}[a-zA-Z0-9]$', kb_name):
                 st.error("çŸ¥è¯†åº“åç§°æ ¼å¼é”™è¯¯ï¼åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦å’Œç‚¹ï¼Œä¸”é•¿åº¦åœ¨3-63ä¹‹é—´ï¼Œé¦–å°¾å¿…é¡»æ˜¯å­—æ¯æˆ–æ•°å­—ã€‚")
            elif not uploaded_files:
                st.error("è¯·å…ˆä¸Šä¼ æ–‡ä»¶")
            else:
                if rag_mode == "äº‘ç«¯ API (SiliconFlow)" and not rag_api_key:
                    st.error("è¯·æä¾› API Key")
                else:
                    with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
                        # ç¡®å®šå‚æ•°
                        e_type = "local" if rag_mode == "æœ¬åœ° (HuggingFace)" else "api"
                        
                        # åˆå§‹åŒ– RAG å¼•æ“
                        st.session_state.rag_engine = init_rag(
                            embedding_type=e_type,
                            model_name=embedding_model_name,
                            api_key=rag_api_key,
                            base_url=rag_base_url
                        )
                    
                        if st.session_state.rag_engine:
                            # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶åˆ°ä¸´æ—¶ç›®å½•
                            temp_dir = tempfile.mkdtemp()
                            file_paths = []
                            for uploaded_file in uploaded_files:
                                file_path = os.path.join(temp_dir, uploaded_file.name)
                                with open(file_path, "wb") as f:
                                    f.write(uploaded_file.getbuffer())
                                file_paths.append(file_path)
                            
                            # åŠ è½½å’Œåˆ‡åˆ†
                            docs = st.session_state.rag_engine.load_documents(file_paths)
                            if isinstance(docs, str): # Error message
                                st.error(docs)
                            else:
                                # æ„å»ºå‘é‡åº“
                                # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ collection nameï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨é»˜è®¤
                                target_collection = kb_name.strip() if kb_name.strip() else "character_data"
                                msg = st.session_state.rag_engine.build_vector_store(docs, collection_name=target_collection)
                                st.success(msg)
                                st.session_state.vector_db_ready = True
                                
                                # ä¿å­˜é…ç½®
                                save_rag_config({
                                    "embedding_type": e_type,
                                    "model_name": embedding_model_name,
                                    "base_url": rag_base_url,
                                    "api_key": rag_api_key # ä¿å­˜ Key
                                })
                        
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        # shutil.rmtree(temp_dir) # å¯ä»¥åœ¨é€‚å½“æ—¶å€™æ¸…ç†

        if st.button("æ¸…ç©ºçŸ¥è¯†åº“"):
            if st.session_state.rag_engine:
                st.session_state.rag_engine.clear_database()
                st.session_state.vector_db_ready = False
                st.success("çŸ¥è¯†åº“å·²æ¸…ç©º")
                # åˆ é™¤é…ç½®æ–‡ä»¶
                if os.path.exists(RAG_CONFIG_FILE):
                    try:
                        os.remove(RAG_CONFIG_FILE)
                    except:
                        pass
                st.rerun()

        # æ˜¾ç¤ºå·²æœ‰çŸ¥è¯†åº“å†…å®¹
        if st.session_state.vector_db_ready and st.session_state.rag_engine:
            st.divider()
            with st.expander("ğŸ“‚ å·²æ”¶å½•æ–‡æ¡£åˆ—è¡¨", expanded=True):
                summary = st.session_state.rag_engine.get_documents_summary()
                if summary:
                    for kb, info in summary.items():
                        files = info['files']
                        count = info['count']
                        st.markdown(f"**ğŸ“¦ {kb}** (å…± {count} ä¸ªç‰‡æ®µ)")
                        for f in files:
                            st.text(f"  â””â”€ ğŸ“„ {f}")
                else:
                    st.caption("æš‚æ— æ–‡ä»¶ä¿¡æ¯")
    # --- ä¸»ç•Œé¢ ---
    tab1, tab2, tab3 = st.tabs(["ğŸ­ è§’è‰²æç¤ºè¯ç”Ÿæˆ", "ğŸ’¬ è‡ªç”±å¯¹è¯", "ğŸ“œ å†å²è®°å½•"])

    # Tab 1: è§’è‰²ç”Ÿæˆ
    with tab1:
        st.markdown("### åŸºäº RAG ç”Ÿæˆè§’è‰² Prompt")
        if not st.session_state.vector_db_ready:
            st.info("è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ æ–‡ä»¶å¹¶æ„å»ºçŸ¥è¯†åº“ã€‚")
        
        # çŸ¥è¯†åº“é€‰æ‹©
        selected_kbs = []
        if st.session_state.vector_db_ready and st.session_state.rag_engine:
            available_kbs = st.session_state.rag_engine.get_available_collections()
            if available_kbs:
                selected_kbs = st.multiselect("é€‰æ‹©æ£€ç´¢èŒƒå›´ï¼ˆçŸ¥è¯†åº“ï¼‰", available_kbs, default=available_kbs)
        
        col1, col2, col3 = st.columns([3, 1, 1])
        with col1:
            char_name = st.text_input("è§’è‰²åç§°", placeholder="ä¾‹å¦‚ï¼šå­™æ‚Ÿç©º")
        with col2:
            char_style = st.selectbox("æç¤ºè¯é£æ ¼", ["è¯¦ç»†è®¾å®šç‰ˆ", "ç®€çŸ­å¯¹è¯ç‰ˆ", "JSONæ ¼å¼"])
        with col3:
            retrieve_k = st.number_input("æ£€ç´¢ç‰‡æ®µæ•°", min_value=1, max_value=20, value=8, help="å¢åŠ æ­¤æ•°å€¼å¯ä»¥è¯»å–æ›´å¤šåŸæ–‡å†…å®¹ï¼Œä½†ä¼šæ¶ˆè€—æ›´å¤š Token")

        extra_req = st.text_area("é¢å¤–è¦æ±‚ (å¯é€‰)", placeholder="ä¾‹å¦‚ï¼šé‡ç‚¹æè¿°ä»–çš„æˆ˜æ–—ç»å†ï¼Œæˆ–è€…ä»–å’ŒæŸäººçš„å…³ç³»...")

        if st.button("ç”Ÿæˆè§’è‰²æç¤ºè¯", disabled=not (st.session_state.vector_db_ready and st.session_state.llm_client)):
            if not char_name:
                st.warning("è¯·è¾“å…¥è§’è‰²åç§°")
            else:
                with st.spinner(f"æ­£åœ¨æ£€ç´¢å…³äº {char_name} çš„ä¿¡æ¯å¹¶ç”Ÿæˆ..."):
                    # 1. RAG æ£€ç´¢
                    query = f"å…³äºè§’è‰² {char_name} çš„å¤–è²Œã€æ€§æ ¼ã€èº«ä¸–ã€è¯´è¯é£æ ¼ã€é‡è¦ç»å†ã€äººé™…å…³ç³»ã€‚"
                    if extra_req:
                        query += f" é¢å¤–å…³æ³¨ï¼š{extra_req}"
                    
                    # ä½¿ç”¨é€‰ä¸­çš„çŸ¥è¯†åº“è¿›è¡Œæ£€ç´¢
                    retrieved_docs = st.session_state.rag_engine.query(query, k=retrieve_k, collection_names=selected_kbs) 
                    
                    context_text = "\n\n".join([doc.page_content for doc in retrieved_docs])
                    
                    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„å†…å®¹ (ç”¨äºè°ƒè¯•/ç¡®è®¤)
                    with st.expander(f"æŸ¥çœ‹æ£€ç´¢åˆ°çš„åŸæ–‡ç‰‡æ®µ (å…± {len(retrieved_docs)} ä¸ªç‰‡æ®µ)"):
                        for i, doc in enumerate(retrieved_docs):
                            st.markdown(f"**ç‰‡æ®µ {i+1}** (Source: {doc.metadata.get('source', 'unknown')}):")
                            # æ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼Œä¸å†æˆªæ–­
                            st.text(doc.page_content)
                            st.divider()

                    # 2. æ„å»º Prompt
                    # å®šä¹‰ç³»ç»Ÿè§’è‰²ï¼šPrompt ä¸“å®¶
                    sys_instruction = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§’è‰²è®¾å®šä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æä¾›çš„èµ„æ–™ï¼Œæ’°å†™æˆ–ä¿®æ”¹å¤§è¯­è¨€æ¨¡å‹çš„è§’è‰²æ‰®æ¼”æç¤ºè¯ï¼ˆSystem Promptï¼‰ã€‚è¯·å§‹ç»ˆä¿æŒå®¢è§‚ã€ä¸“ä¸šçš„æ€åº¦ï¼Œç›´æ¥è¾“å‡ºä¼˜åŒ–åçš„ Promptï¼Œä¸è¦è¿›è¡Œè§’è‰²æ‰®æ¼”ï¼Œä¹Ÿä¸è¦è¾“å‡ºæ— å…³çš„é—²èŠã€‚"
                    
                    user_task = f"""ç›®æ ‡è§’è‰²ï¼š{char_name}
é£æ ¼è¦æ±‚ï¼š{char_style}

è¯·ä»ä»¥ä¸‹åŸæ–‡ç‰‡æ®µä¸­æå–ä¿¡æ¯ï¼š
1. å¤–è²Œç‰¹å¾
2. æ€§æ ¼ç‰¹ç‚¹ï¼ˆåŒ…æ‹¬ä¼˜ç‚¹å’Œç¼ºç‚¹ï¼‰
3. è¯´è¯é£æ ¼ï¼ˆå£ç™–ã€è¯­æ°”ã€å¸¸ç”¨è¯ï¼‰
4. èƒŒæ™¯æ•…äº‹å’Œé‡è¦ç»å†
5. äººé™…å…³ç³»

åŸæ–‡ç‰‡æ®µï¼š
{context_text}

ç”¨æˆ·é¢å¤–è¦æ±‚ï¼š{extra_req}

è¯·è¾“å‡ºä¸€ä¸ªç»“æ„æ¸…æ™°ã€å¯ä»¥ç›´æ¥å¤åˆ¶ä½¿ç”¨çš„ System Promptã€‚å¦‚æœåŸæ–‡ä¿¡æ¯ä¸è¶³ï¼Œè¯·æ ¹æ®è§’è‰²è®¾å®šè¿›è¡Œåˆç†çš„é€»è¾‘æ¨æ–­ï¼Œä½†ä¸è¦æé€ ä¸åŸæ–‡å†²çªçš„äº‹å®ã€‚
"""
                    # é‡ç½®å¯¹è¯å†å²
                    st.session_state.gen_messages = [
                        {"role": "system", "content": sys_instruction},
                        {"role": "user", "content": user_task}
                    ]

                    # 3. è°ƒç”¨ LLM
                    full_response = ""
                    
                    with st.chat_message("assistant"):
                        # æµå¼è¾“å‡º
                        stream = st.session_state.llm_client.chat(st.session_state.gen_messages, model=selected_model, stream=True)
                        
                        if isinstance(stream, str): # Error
                            st.error(stream)
                        else:
                            response_placeholder = st.empty()
                            for chunk in stream:
                                if chunk.choices[0].delta.content:
                                    content = chunk.choices[0].delta.content
                                    full_response += content
                                    response_placeholder.markdown(full_response)
                            
                            st.session_state.gen_messages.append({"role": "assistant", "content": full_response})
                            st.rerun()

        # æ˜¾ç¤ºç”Ÿæˆå†å²å’Œå¯¹è¯
        for msg in st.session_state.gen_messages:
            if msg["role"] == "system":
                continue # ä¸æ˜¾ç¤ºç³»ç»ŸæŒ‡ä»¤
            if msg["role"] == "user":
                # éšè—åˆå§‹çš„å¤§æ®µ Promptï¼Œåªæ˜¾ç¤ºåç»­çš„ä¿®æ”¹æ„è§
                if msg["content"].startswith("ç›®æ ‡è§’è‰²ï¼š"):
                    with st.expander("æŸ¥çœ‹åˆå§‹ Prompt è¯·æ±‚"):
                        st.text(msg["content"])
                else:
                    with st.chat_message("user"):
                        st.markdown(msg["content"])
            else:
                with st.chat_message("assistant"):
                    st.markdown(msg["content"])

        # ä¿å­˜æŒ‰é’®
        if st.session_state.gen_messages and st.session_state.gen_messages[-1]["role"] == "assistant":
            if st.button("ğŸ’¾ ä¿å­˜å½“å‰ Prompt åˆ°å†å²è®°å½•"):
                last_response = st.session_state.gen_messages[-1]["content"]
                save_history_item(char_name, last_response)
                st.success("å·²ä¿å­˜ï¼")

        # ä¿®æ”¹æ„è§è¾“å…¥æ¡†
        if prompt := st.chat_input("å¯¹ç»“æœä¸æ»¡æ„ï¼Ÿè¯·è¾“å…¥ä¿®æ”¹æ„è§...", key="gen_chat"):
            if not st.session_state.gen_messages:
                st.warning("è¯·å…ˆç”Ÿæˆè§’è‰²æç¤ºè¯")
            else:
                st.session_state.gen_messages.append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    st.markdown(prompt)

                with st.chat_message("assistant"):
                    stream = st.session_state.llm_client.chat(st.session_state.gen_messages, model=selected_model, stream=True)
                    if isinstance(stream, str):
                        st.error(stream)
                    else:
                        full_response = ""
                        response_placeholder = st.empty()
                        for chunk in stream:
                            if chunk.choices[0].delta.content:
                                content = chunk.choices[0].delta.content
                                full_response += content
                                response_placeholder.markdown(full_response)
                        st.session_state.gen_messages.append({"role": "assistant", "content": full_response})

    # Tab 2: è‡ªç”±å¯¹è¯
    with tab2:
        st.markdown("### ä¸æ¨¡å‹å¯¹è¯ (å¯é€‰ RAG)")
        enable_rag = st.checkbox("å¯ç”¨ RAG (å¼•ç”¨çŸ¥è¯†åº“)", value=True, disabled=not st.session_state.vector_db_ready)
        
        # æ˜¾ç¤ºå†å²æ¶ˆæ¯
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        if prompt := st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜..."):
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            # ç”Ÿæˆå›å¤
            with st.chat_message("assistant"):
                if not st.session_state.llm_client:
                    st.error("è¯·å…ˆé…ç½® API Key")
                else:
                    context_str = ""
                    if enable_rag and st.session_state.vector_db_ready:
                        with st.spinner("æ£€ç´¢ä¸­..."):
                            docs = st.session_state.rag_engine.query(prompt, k=3)
                            context_str = "\n\n".join([doc.page_content for doc in docs])
                            with st.expander("å‚è€ƒä¸Šä¸‹æ–‡"):
                                st.text(context_str)
                    
                    # æ„å»ºæ¶ˆæ¯
                    messages_payload = []
                    # å¦‚æœæœ‰ RAG ä¸Šä¸‹æ–‡ï¼Œæ’å…¥åˆ° System Prompt æˆ– User Prompt ä¸­
                    if context_str:
                        system_msg = f"ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n\nä¸Šä¸‹æ–‡ï¼š\n{context_str}"
                        messages_payload.append({"role": "system", "content": system_msg})
                    
                    # æ·»åŠ å†å²è®°å½• (ç®€å•å¤„ç†ï¼Œåªå–æœ€è¿‘å‡ è½®ä»¥èŠ‚çœ token)
                    for m in st.session_state.messages[-5:]:
                        messages_payload.append(m)
                    
                    # å¦‚æœæ²¡æœ‰ RAG ä¸”æ²¡æœ‰å†å² system promptï¼Œå¯ä»¥åŠ ä¸€ä¸ªé»˜è®¤çš„
                    if not context_str and not any(m['role'] == 'system' for m in messages_payload):
                         messages_payload.insert(0, {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚"})

                    # è°ƒç”¨ LLM
                    response_placeholder = st.empty()
                    full_response = ""
                    stream = st.session_state.llm_client.chat(messages_payload, model=selected_model, stream=True)
                    
                    if isinstance(stream, str):
                        st.error(stream)
                    else:
                        for chunk in stream:
                            if chunk.choices[0].delta.content:
                                content = chunk.choices[0].delta.content
                                full_response += content
                                response_placeholder.markdown(full_response)
                        
                        st.session_state.messages.append({"role": "assistant", "content": full_response})

    # Tab 3: å†å²è®°å½•
    with tab3:
        st.markdown("### ğŸ“œ å†å² Prompt è®°å½•")
        history = load_history()
        if not history:
            st.info("æš‚æ— å†å²è®°å½•ã€‚")
        else:
            for i, item in enumerate(history):
                with st.expander(f"{item['timestamp']} - {item['char_name']}"):
                    st.code(item['content'], language="markdown")
                    if st.button("åˆ é™¤", key=f"del_{i}"):
                        delete_history_item(i)
                        st.rerun()

if __name__ == "__main__":
    main()
