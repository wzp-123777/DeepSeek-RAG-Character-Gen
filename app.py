import streamlit as st
import os
import tempfile
import json
import re

# è®¾ç½® HuggingFace é•œåƒï¼Œè§£å†³å›½å†…è¿æ¥é—®é¢˜
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'

from llm_client import LLMClient
from rag_engine import RAGEngine
from history_utils import save_history_item, load_history, delete_history_item

RAG_CONFIG_FILE = "rag_config.json"

def save_rag_config(config):
    try:
        with open(RAG_CONFIG_FILE, "w", encoding="utf-8") as f:
            json.dump(config, f)
    except Exception as e:
        print(f"ä¿å­˜é…ç½®å¤±è´¥: {e}")

def load_rag_config():
    if os.path.exists(RAG_CONFIG_FILE):
        try:
            with open(RAG_CONFIG_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            pass
    return None

# é¡µé¢é…ç½®
st.set_page_config(page_title="DeepSeek RAG è§’è‰²ç”Ÿæˆå™¨", layout="wide")

# åˆå§‹åŒ– Session State
if "messages" not in st.session_state:
    st.session_state.messages = []
if "gen_messages" not in st.session_state:
    st.session_state.gen_messages = []
if "qq_dialogue_messages" not in st.session_state:
    st.session_state.qq_dialogue_messages = []
if "qq_prompt_data" not in st.session_state:
    st.session_state.qq_prompt_data = {
        "character_info": "",
        "background": "",
        "chat_requirements": "",
        "dialogue_examples": [
            {"user": "ä½ å¥½", "character": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„å—ï¼Ÿ"},
            {"user": "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ", "character": "ä»Šå¤©å¤©æ°”ä¸é”™ï¼Œé˜³å…‰æ˜åªšã€‚"},
            {"user": "ä½ æœ€å–œæ¬¢åšä»€ä¹ˆï¼Ÿ", "character": "æˆ‘å–œæ¬¢èŠå¤©å’Œå¸®åŠ©åˆ«äººã€‚"},
            {"user": "è®²ä¸ªç¬‘è¯å§", "character": "ä¸ºä»€ä¹ˆé¸¡è¿‡é©¬è·¯ï¼Ÿå› ä¸ºå¯¹é¢æœ‰è‚¯å¾·åŸºï¼"},
            {"user": "å†è§", "character": "å†è§ï¼Œæœ‰äº‹éšæ—¶æ‰¾æˆ‘ï¼"}
        ]
    }
if "rag_engine" not in st.session_state:
    st.session_state.rag_engine = None
if "llm_client" not in st.session_state:
    st.session_state.llm_client = None
if "vector_db_ready" not in st.session_state:
    st.session_state.vector_db_ready = False

def init_rag(embedding_type, model_name, api_key=None, base_url=None):
    try:
        return RAGEngine(
            embedding_type=embedding_type, 
            model_name=model_name,
            api_key=api_key,
            base_url=base_url
        )
    except Exception as e:
        st.error(f"åˆå§‹åŒ– RAG å¼•æ“å¤±è´¥: {e}")
        return None

# å°è¯•è‡ªåŠ¨åŠ è½½æœ¬åœ°çŸ¥è¯†åº“é…ç½®
if not st.session_state.rag_engine and os.path.exists("./chroma_db") and os.path.exists(RAG_CONFIG_FILE):
    config = load_rag_config()
    if config:
        e_type = config.get("embedding_type")
        api_key_to_use = config.get("api_key")
        
        # å¦‚æœæ˜¯ API æ¨¡å¼ä¸”æ²¡æœ‰ä¿å­˜ Keyï¼Œå°è¯•ä»ç”¨æˆ·é…ç½®è¯»å–ï¼ˆå‡è®¾å¤ç”¨ï¼‰
        if e_type == "api" and not api_key_to_use:
             # è¿™é‡Œéœ€è¦ä¸´æ—¶åŠ è½½ä¸€ä¸‹ user config
             if os.path.exists("user_config.json"):
                 try:
                     with open("user_config.json", "r", encoding="utf-8") as f:
                         user_conf = json.load(f)
                         api_key_to_use = user_conf.get("api_key")
                 except:
                     pass

        if e_type == "local" or (e_type == "api" and api_key_to_use):
            with st.spinner("æ­£åœ¨è‡ªåŠ¨åŠ è½½ä¸Šæ¬¡çš„çŸ¥è¯†åº“..."):
                st.session_state.rag_engine = init_rag(
                    embedding_type=e_type,
                    model_name=config["model_name"],
                    api_key=api_key_to_use,
                    base_url=config.get("base_url")
                )
                if st.session_state.rag_engine:
                    # ç®€å•çš„éªŒè¯ä¸€ä¸‹æ˜¯å¦çœŸçš„æœ‰æ•°æ®
                    # è¿™é‡Œä¸è¿›è¡Œæ·±å±‚æ£€æŸ¥ï¼Œå‡è®¾ chroma_db å­˜åœ¨å³æœ‰æ•ˆ
                    st.session_state.vector_db_ready = True
                    # st.toast("å·²è‡ªåŠ¨åŠ è½½ä¸Šæ¬¡çš„çŸ¥è¯†åº“") # toast åœ¨è¿™é‡Œå¯èƒ½æ˜¾ç¤ºä¸å‡ºæ¥ï¼Œå› ä¸ºè¿˜æ²¡æ¸²æŸ“é¡µé¢

USER_CONFIG_FILE = "user_config.json"

def save_user_config(config):
    try:
        with open(USER_CONFIG_FILE, "w", encoding="utf-8") as f:
            json.dump(config, f)
    except Exception as e:
        print(f"ä¿å­˜ç”¨æˆ·é…ç½®å¤±è´¥: {e}")

def load_user_config():
    if os.path.exists(USER_CONFIG_FILE):
        try:
            with open(USER_CONFIG_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except:
            pass
    return {}

def update_qq_prompt_data():
    """æ›´æ–°QQ promptæ•°æ®"""
    st.session_state.qq_prompt_data["character_info"] = st.session_state.edit_character_info
    st.session_state.qq_prompt_data["background"] = st.session_state.edit_background
    st.session_state.qq_prompt_data["chat_requirements"] = st.session_state.edit_chat_requirements

def update_example(index, field):
    """æ›´æ–°å¯¹è¯ç¤ºä¾‹"""
    key = f"{'user' if field == 'user' else 'char'}_msg_{index}"
    if key in st.session_state:
        st.session_state.qq_prompt_data["dialogue_examples"][index][field] = st.session_state[key]

def main():
    st.title("ğŸ¤– DeepSeek RAG è§’è‰²æç¤ºè¯ç”Ÿæˆå™¨")
    
    # åŠ è½½ç”¨æˆ·é…ç½®
    user_config = load_user_config()
    
    # --- ä¾§è¾¹æ é…ç½® ---
    with st.sidebar:
        st.header("âš™ï¸ è®¾ç½®")
        
        # API é…ç½®
        default_provider_index = 0
        if user_config.get("api_provider") == "siliconflow":
            default_provider_index = 1
            
        api_provider = st.selectbox("é€‰æ‹© LLM æä¾›å•†", ["deepseek", "siliconflow"], index=default_provider_index)
        
        default_api_key = user_config.get("api_key", "")
        api_key = st.text_input("API Key", value=default_api_key, type="password", help="è¾“å…¥å¯¹åº”çš„ API Key")
        
        # åˆå§‹åŒ– LLM Client
        if api_key:
            try:
                st.session_state.llm_client = LLMClient(provider=api_provider, api_key=api_key)
                models = st.session_state.llm_client.get_available_models()
                
                default_model_index = 0
                saved_model = user_config.get("model_name")
                if saved_model and saved_model in models:
                    default_model_index = models.index(saved_model)
                
                selected_model = st.selectbox("é€‰æ‹©å¯¹è¯æ¨¡å‹", models, index=default_model_index)
                st.success(f"å·²è¿æ¥åˆ° {api_provider}")
                
                # ä¿å­˜é…ç½®ï¼ˆå½“è¿æ¥æˆåŠŸæ—¶ï¼‰
                if api_key != user_config.get("api_key") or api_provider != user_config.get("api_provider") or selected_model != user_config.get("model_name"):
                    save_user_config({
                        "api_provider": api_provider,
                        "api_key": api_key,
                        "model_name": selected_model
                    })
                    
            except Exception as e:
                st.error(f"è¿æ¥å¤±è´¥: {e}")
        else:
            st.warning("è¯·è¾“å…¥ API Key ä»¥ç»§ç»­")

        st.divider()

        # RAG é…ç½®
        st.subheader("ğŸ“š çŸ¥è¯†åº“æ„å»º")
        
        rag_config = load_rag_config() or {}
        
        # é»˜è®¤é€‰ä¸­ä¸Šæ¬¡çš„æ¨¡å¼
        default_mode_index = 0
        if rag_config.get("embedding_type") == "api":
            default_mode_index = 1
            
        rag_mode = st.radio("Embedding æ¨¡å¼", ["æœ¬åœ° (HuggingFace)", "äº‘ç«¯ API (SiliconFlow)"], index=default_mode_index)
        
        embedding_model_name = ""
        rag_api_key = None
        rag_base_url = None
        
        if rag_mode == "æœ¬åœ° (HuggingFace)":
            default_model = rag_config.get("model_name", "sentence-transformers/all-MiniLM-L6-v2")
            if rag_config.get("embedding_type") != "local": # å¦‚æœä¸Šæ¬¡ä¸æ˜¯ localï¼Œå°±ç”¨é»˜è®¤å€¼
                 default_model = "sentence-transformers/all-MiniLM-L6-v2"
                 
            embedding_model_name = st.text_input("æ¨¡å‹åç§°", value=default_model)
            st.caption("æç¤ºï¼šé¦–æ¬¡è¿è¡Œä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ã€‚å·²é…ç½®å›½å†…é•œåƒåŠ é€Ÿã€‚")
        else:
            # å°è¯•æ‰¾åˆ°ä¸Šæ¬¡ä½¿ç”¨çš„æ¨¡å‹ index
            model_options = [
                    "BAAI/bge-m3", 
                    "BAAI/bge-large-zh-v1.5", 
                    "Qwen/Qwen3-Embedding-8B", # ç”¨æˆ·æŒ‡å®š
                    "netease-youdao/bce-embedding-base_v1"
                ]
            default_emb_index = 0
            if rag_config.get("embedding_type") == "api" and rag_config.get("model_name") in model_options:
                default_emb_index = model_options.index(rag_config.get("model_name"))
                
            embedding_model_name = st.selectbox(
                "é€‰æ‹© Embedding æ¨¡å‹", 
                model_options,
                index=default_emb_index
            )
            st.caption("æ¨èä½¿ç”¨ BAAI/bge-m3 æˆ– Qwen/Qwen3-Embedding-8B")
            # é»˜è®¤å¤ç”¨ä¸Šé¢çš„ API Keyï¼Œå¦‚æœç”¨æˆ·éœ€è¦å•ç‹¬è®¾ç½®ä¹Ÿå¯ä»¥
            use_same_key = st.checkbox("ä½¿ç”¨ä¸Šæ–¹ç›¸åŒçš„ API Key", value=True)
            if use_same_key:
                rag_api_key = api_key
            else:
                rag_api_key = st.text_input("Embedding API Key", type="password")
            
            rag_base_url = "https://api.siliconflow.cn/v1"

        uploaded_files = st.file_uploader("ä¸Šä¼ å¤§æ–‡æœ¬ (txt, pdf, docx)", accept_multiple_files=True)
        
        # æ–°å¢ï¼šç½‘é¡µ URL è¾“å…¥
        st.markdown("æˆ–è€…")
        input_urls = st.text_area("è¾“å…¥ç½‘é¡µé“¾æ¥ (æ¯è¡Œä¸€ä¸ª)", height=100, help="æ”¯æŒç›´æ¥è¯»å–ç½‘é¡µå°è¯´ç« èŠ‚å†…å®¹")
        is_crawl_mode = st.checkbox("è¿™æ˜¯ä¸€ä¸ªç›®å½•é¡µ (è‡ªåŠ¨æŠ“å–é¡µé¢å†…çš„ç« èŠ‚é“¾æ¥)", value=False, help="å‹¾é€‰åï¼Œç³»ç»Ÿä¼šå°è¯•åˆ†æé¡µé¢ä¸­çš„é“¾æ¥ï¼Œå¹¶æŠ“å–æ‰€æœ‰ç« èŠ‚å†…å®¹ã€‚")
        
        kb_name = st.text_input("ç›®æ ‡çŸ¥è¯†åº“åç§° (ä»…é™å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿)", value="default_kb", help="å°†æ–‡ä»¶å­˜å…¥æŒ‡å®šçš„çŸ¥è¯†åº“åˆ†ç»„ä¸­ã€‚æ³¨æ„ï¼šä¸æ”¯æŒä¸­æ–‡ï¼Œé•¿åº¦3-63å­—ç¬¦ã€‚")
        
        if st.button("æ„å»º/æ›´æ–° çŸ¥è¯†åº“"):
            # æ ¡éªŒçŸ¥è¯†åº“åç§°
            if not re.match(r'^[a-zA-Z0-9][a-zA-Z0-9._-]{1,61}[a-zA-Z0-9]$', kb_name):
                 st.error("çŸ¥è¯†åº“åç§°æ ¼å¼é”™è¯¯ï¼åªèƒ½åŒ…å«å­—æ¯ã€æ•°å­—ã€ä¸‹åˆ’çº¿ã€è¿å­—ç¬¦å’Œç‚¹ï¼Œä¸”é•¿åº¦åœ¨3-63ä¹‹é—´ï¼Œé¦–å°¾å¿…é¡»æ˜¯å­—æ¯æˆ–æ•°å­—ã€‚")
            elif not uploaded_files and not input_urls.strip():
                st.error("è¯·å…ˆä¸Šä¼ æ–‡ä»¶æˆ–è¾“å…¥ç½‘é¡µé“¾æ¥")
            else:
                if rag_mode == "äº‘ç«¯ API (SiliconFlow)" and not rag_api_key:
                    st.error("è¯·æä¾› API Key")
                else:
                    with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£..."):
                        # ç¡®å®šå‚æ•°
                        e_type = "local" if rag_mode == "æœ¬åœ° (HuggingFace)" else "api"
                        
                        # åˆå§‹åŒ– RAG å¼•æ“
                        st.session_state.rag_engine = init_rag(
                            embedding_type=e_type,
                            model_name=embedding_model_name,
                            api_key=rag_api_key,
                            base_url=rag_base_url
                        )
                    
                        if st.session_state.rag_engine:
                            all_docs = []
                            
                            # 1. å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶
                            if uploaded_files:
                                temp_dir = tempfile.mkdtemp()
                                file_paths = []
                                for uploaded_file in uploaded_files:
                                    file_path = os.path.join(temp_dir, uploaded_file.name)
                                    with open(file_path, "wb") as f:
                                        f.write(uploaded_file.getbuffer())
                                    file_paths.append(file_path)
                                
                                file_docs = st.session_state.rag_engine.load_documents(file_paths)
                                if isinstance(file_docs, str):
                                    st.error(f"æ–‡ä»¶å¤„ç†é”™è¯¯: {file_docs}")
                                else:
                                    all_docs.extend(file_docs)
                                    
                            # 2. å¤„ç†ç½‘é¡µé“¾æ¥
                            if input_urls.strip():
                                url_list = [url.strip() for url in input_urls.split('\n') if url.strip()]
                                if url_list:
                                    web_docs = st.session_state.rag_engine.load_urls(url_list, fetch_links=is_crawl_mode)
                                    if isinstance(web_docs, str):
                                        st.error(f"ç½‘é¡µå¤„ç†é”™è¯¯: {web_docs}")
                                    else:
                                        all_docs.extend(web_docs)

                            if not all_docs:
                                st.warning("æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹ã€‚")
                            else:
                                # æ„å»ºå‘é‡åº“
                                # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ collection nameï¼Œå¦‚æœä¸ºç©ºåˆ™ä½¿ç”¨é»˜è®¤
                                target_collection = kb_name.strip() if kb_name.strip() else "character_data"
                                msg = st.session_state.rag_engine.build_vector_store(all_docs, collection_name=target_collection)
                                st.success(msg)
                                st.session_state.vector_db_ready = True
                                
                                # ä¿å­˜é…ç½®
                                save_rag_config({
                                    "embedding_type": e_type,
                                    "model_name": embedding_model_name,
                                    "base_url": rag_base_url,
                                    "api_key": rag_api_key # ä¿å­˜ Key
                                })
                        
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        # shutil.rmtree(temp_dir) # å¯ä»¥åœ¨é€‚å½“æ—¶å€™æ¸…ç†

        # --- çŸ¥è¯†åº“ç®¡ç†åŒºåŸŸ ---
        st.divider()
        st.subheader("ğŸ—‘ï¸ çŸ¥è¯†åº“ç®¡ç†")
        
        if st.session_state.vector_db_ready and st.session_state.rag_engine:
            available_kbs = st.session_state.rag_engine.get_available_collections()
            
            # 1. åˆ é™¤å•ä¸ªçŸ¥è¯†åº“
            if available_kbs:
                col_del1, col_del2 = st.columns([3, 1])
                with col_del1:
                    kb_to_delete = st.selectbox("é€‰æ‹©è¦åˆ é™¤çš„çŸ¥è¯†åº“", [""] + available_kbs, key="del_kb_select")
                with col_del2:
                    if kb_to_delete and st.button("åˆ é™¤", key="del_kb_btn"):
                        success, msg = st.session_state.rag_engine.delete_collection(kb_to_delete)
                        if success:
                            st.success(msg)
                            st.rerun()
                        else:
                            st.error(msg)
            
            # 2. æ¸…ç©ºæ‰€æœ‰
            if st.button("âš ï¸ æ¸…ç©ºæ‰€æœ‰çŸ¥è¯†åº“", type="primary"):
                if st.session_state.rag_engine:
                    st.session_state.rag_engine.clear_database()
                    st.session_state.vector_db_ready = False
                    st.success("çŸ¥è¯†åº“å·²å…¨éƒ¨æ¸…ç©º")
                    # åˆ é™¤é…ç½®æ–‡ä»¶
                    if os.path.exists(RAG_CONFIG_FILE):
                        try:
                            os.remove(RAG_CONFIG_FILE)
                        except:
                            pass
                    st.rerun()

        # æ˜¾ç¤ºå·²æœ‰çŸ¥è¯†åº“å†…å®¹
        if st.session_state.vector_db_ready and st.session_state.rag_engine:
            st.divider()
            with st.expander("ğŸ“‚ å·²æ”¶å½•æ–‡æ¡£åˆ—è¡¨", expanded=False):
                summary = st.session_state.rag_engine.get_documents_summary()
                if summary:
                    for kb, info in summary.items():
                        files = info['files']
                        count = info['count']
                        st.markdown(f"**ğŸ“¦ {kb}** (å…± {count} ä¸ªç‰‡æ®µ)")
                        for f in files:
                            st.text(f"  â””â”€ ğŸ“„ {f}")
                else:
                    st.caption("æš‚æ— æ–‡ä»¶ä¿¡æ¯")
    # --- ä¸»ç•Œé¢ ---
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ­ è§’è‰²æç¤ºè¯ç”Ÿæˆ", "ğŸ’¬ è‡ªç”±å¯¹è¯", "ğŸ¤– QQè§’è‰²ç”Ÿæˆ", "ğŸ“œ å†å²è®°å½•"])

    # Tab 1: è§’è‰²ç”Ÿæˆ
    with tab1:
        st.markdown("### åŸºäº RAG ç”Ÿæˆè§’è‰² Prompt")
        if not st.session_state.vector_db_ready:
            st.info("è¯·å…ˆåœ¨å·¦ä¾§ä¸Šä¼ æ–‡ä»¶å¹¶æ„å»ºçŸ¥è¯†åº“ã€‚")
        
        # çŸ¥è¯†åº“é€‰æ‹©
        selected_kbs = []
        if st.session_state.vector_db_ready and st.session_state.rag_engine:
            available_kbs = st.session_state.rag_engine.get_available_collections()
            if available_kbs:
                # ç®€å•çš„åˆ†ç±»é€»è¾‘ï¼šå°è¯•æå–å‰ç¼€ï¼ˆæŒ‰ _ æˆ– - åˆ†å‰²ï¼‰
                prefixes = set()
                for kb in available_kbs:
                    if "_" in kb:
                        prefixes.add(kb.split("_")[0])
                    elif "-" in kb:
                        prefixes.add(kb.split("-")[0])
                
                # å¦‚æœæœ‰å‰ç¼€åˆ†ç±»ï¼Œæ˜¾ç¤ºå¿«é€Ÿç­›é€‰
                default_selection = available_kbs
                if prefixes:
                    cols_filter = st.columns([1, 2])
                    with cols_filter[0]:
                        filter_category = st.selectbox("ğŸ“‚ æŒ‰å¤§ç±»å¿«é€Ÿç­›é€‰", ["å…¨éƒ¨"] + sorted(list(prefixes)), help="æ ¹æ®çŸ¥è¯†åº“åç§°çš„å‰ç¼€ï¼ˆå¦‚ 'å°è¯´A_ç¬¬ä¸€å·' ä¸­çš„ 'å°è¯´A'ï¼‰è¿›è¡Œç­›é€‰")
                    
                    if filter_category != "å…¨éƒ¨":
                        default_selection = [kb for kb in available_kbs if kb.startswith(filter_category)]
                
                selected_kbs = st.multiselect("é€‰æ‹©æ£€ç´¢èŒƒå›´ï¼ˆçŸ¥è¯†åº“ï¼‰", available_kbs, default=default_selection)
        
        col1, col2, col3 = st.columns([3, 1, 1])
        with col1:
            char_name = st.text_input("è§’è‰²åç§°", placeholder="ä¾‹å¦‚ï¼šå­™æ‚Ÿç©º")
        with col2:
            char_style = st.selectbox("æç¤ºè¯é£æ ¼", ["è¯¦ç»†è®¾å®šç‰ˆ", "ç®€çŸ­å¯¹è¯ç‰ˆ", "JSONæ ¼å¼"])
        with col3:
            retrieve_k = st.number_input("æ£€ç´¢ç‰‡æ®µæ•°", min_value=1, max_value=100, value=15, help="å¢åŠ æ­¤æ•°å€¼å¯ä»¥è¯»å–æ›´å¤šåŸæ–‡å†…å®¹ï¼Œä½†ä¼šæ¶ˆè€—æ›´å¤š Token")

        extra_req = st.text_area("é¢å¤–è¦æ±‚ (å¯é€‰)", placeholder="ä¾‹å¦‚ï¼šé‡ç‚¹æè¿°ä»–çš„æˆ˜æ–—ç»å†ï¼Œæˆ–è€…ä»–å’ŒæŸäººçš„å…³ç³»...")

        if st.button("ç”Ÿæˆè§’è‰²æç¤ºè¯", disabled=not (st.session_state.vector_db_ready and st.session_state.llm_client)):
            if not char_name:
                st.warning("è¯·è¾“å…¥è§’è‰²åç§°")
            else:
                with st.spinner(f"æ­£åœ¨å¤šè§’åº¦æ£€ç´¢å…³äº {char_name} çš„ä¿¡æ¯..."):
                    # 1. RAG å¤šè·¯æ£€ç´¢ (Multi-Query Retrieval)
                    # å®šä¹‰ä¸åŒçš„æ£€ç´¢è§’åº¦ï¼Œä»¥æå–æ›´ä¸°å¯Œçš„ä¿¡æ¯
                    queries = [
                        f"å…³äºè§’è‰² {char_name} çš„å¤–è²Œæå†™ã€æ€§æ ¼ç‰¹å¾ã€èº«ä¸–èƒŒæ™¯",
                        f"{char_name} çš„è¯´è¯é£æ ¼ã€å£å¤´ç¦…ã€ç»å…¸å°è¯ã€è¯­æ°”",
                        f"{char_name} çš„é‡è¦ç»å†ã€å…³é”®å‰§æƒ…ã€äººé™…å…³ç³»ã€å¯¹å…¶ä»–äººçš„æ€åº¦"
                    ]
                    if extra_req:
                        queries.append(f"{char_name} {extra_req}")
                    
                    all_retrieved_docs = []
                    seen_contents = set()
                    
                    # æ‰§è¡Œå¤šæ¬¡æ£€ç´¢
                    for q in queries:
                        docs = st.session_state.rag_engine.query(q, k=retrieve_k, collection_names=selected_kbs)
                        for doc in docs:
                            if doc.page_content not in seen_contents:
                                seen_contents.add(doc.page_content)
                                all_retrieved_docs.append(doc)
                    
                    # æˆªå–ç”¨æˆ·æŒ‡å®šçš„æ•°é‡ (å¦‚æœå¤šè·¯æ£€ç´¢ç»“æœå¤ªå¤š)
                    # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šä¼˜å…ˆä¿ç•™å‰é¢çš„ç»“æœï¼ˆé€šå¸¸ç›¸å…³æ€§æ›´é«˜ï¼‰ï¼Œä½†å› ä¸ºæˆ‘ä»¬æ˜¯å¤šè·¯åˆå¹¶ï¼Œ
                    # ç®€å•çš„æˆªæ–­å¯èƒ½ä¸å¤Ÿå®Œç¾ï¼Œä½†å¯¹äº RAG æ¥è¯´ï¼Œå»é‡åçš„å¹¶é›†é€šå¸¸æ˜¯æœ€å¥½çš„ã€‚
                    # å¦‚æœæ•°é‡å®åœ¨å¤ªå¤šè¶…è¿‡ retrieve_k * 2ï¼Œå¯ä»¥é€‚å½“æˆªæ–­ï¼Œé˜²æ­¢ Token çˆ†ç‚¸
                    if len(all_retrieved_docs) > retrieve_k:
                         # è¿™é‡Œæˆ‘ä»¬ç¨å¾®æ”¾å®½ä¸€ç‚¹ï¼Œå…è®¸æ¯”ç”¨æˆ·è®¾å®šçš„å¤šä¸€ç‚¹ï¼Œå› ä¸ºæ˜¯å¤šè·¯åˆå¹¶çš„
                         all_retrieved_docs = all_retrieved_docs[:retrieve_k]

                    context_text = "\n\n".join([doc.page_content for doc in all_retrieved_docs])
                    
                    # æ˜¾ç¤ºæ£€ç´¢åˆ°çš„å†…å®¹ (ç”¨äºè°ƒè¯•/ç¡®è®¤)
                    with st.expander(f"æŸ¥çœ‹æ£€ç´¢åˆ°çš„åŸæ–‡ç‰‡æ®µ (å…± {len(all_retrieved_docs)} ä¸ªç‰‡æ®µ)"):
                        st.info("å·²å¯ç”¨å¤šè§’åº¦æ··åˆæ£€ç´¢ï¼ˆå¤–è²Œæ€§æ ¼ + è¯­è¨€é£æ ¼ + ç»å†å…³ç³» + é¢å¤–è¦æ±‚ï¼‰")
                        for i, doc in enumerate(all_retrieved_docs):
                            st.markdown(f"**ç‰‡æ®µ {i+1}** (Source: {doc.metadata.get('source', 'unknown')}):")
                            # æ˜¾ç¤ºå®Œæ•´å†…å®¹ï¼Œä¸å†æˆªæ–­
                            st.text(doc.page_content)
                            st.divider()

                    # 2. æ„å»º Prompt (ç¬¬ä¸€é˜¶æ®µï¼šç”Ÿæˆ)
                    gen_prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§’è‰²è®¾å®šä¸“å®¶ã€‚è¯·æ ¹æ®æä¾›çš„åŸæ–‡ç‰‡æ®µï¼Œä¸ºè§’è‰²ã€{char_name}ã€‘æ’°å†™ä¸€ä»½é«˜çº§çš„è§’è‰²æ‰®æ¼” System Promptã€‚

ã€ä»»åŠ¡è¦æ±‚ã€‘
1. **Promptç»“æ„**ï¼šè¯·ä½¿ç”¨åŠ¨æ€Promptç»“æ„ï¼ŒåŒ…å«ä»¥ä¸‹æ¨¡å—ï¼š
   - [è§’è‰²è¯¦æƒ…]ï¼šå§“åã€å¹´é¾„ã€èº«ä»½ç­‰ã€‚
   - [æ€§æ ¼ç‰¹è´¨]ï¼šæ·±å±‚æ€§æ ¼ã€è¡Œäº‹é€»è¾‘ã€ä¼˜ç¼ºç‚¹ã€‚
   - [è¯­è¨€é£æ ¼]ï¼šå£ç™–ã€è¯­æ°”ã€å¸¸ç”¨è¯ã€å¥å¼ç‰¹ç‚¹ã€‚
   - [ç»å†èƒŒæ™¯]ï¼šå…³é”®èº«ä¸–ã€é‡è¦å‰§æƒ…èŠ‚ç‚¹ã€‚
   - [äººé™…å…³ç³»]ï¼šä¸å…³é”®äººç‰©çš„å…³ç³»åŠæ€åº¦ã€‚
2. **å¯¹è¯ç”Ÿæˆ**ï¼šè¯·ç”Ÿæˆä¸€æ®µåŒ…å« **5ä¸ªæ¥å›** çš„å¯¹è¯ç¤ºä¾‹ï¼ˆUserä¸{char_name}çš„äº’åŠ¨ï¼‰ã€‚å¯¹è¯å†…å®¹éœ€ç´§æ‰£å‰§æƒ…é€»è¾‘ï¼Œå±•ç°è§’è‰²çš„è¯­æ°”å’Œæ€§æ ¼ã€‚
3. **è¡Œæ–‡é£æ ¼æå–**ï¼š**å¿…é¡»**åœ¨æ‰€æœ‰è¾“å‡ºçš„æœ€åï¼Œå•ç‹¬åˆ—å‡ºä¸€ä¸ªç« èŠ‚å«â€œã€æå–çš„åŸæ–‡æœ¬è¡Œæ–‡é£æ ¼ã€‘â€ï¼Œæè¿°åŸæ–‡çš„æå†™æ‰‹æ³•ã€ä¿®è¾é£æ ¼å’Œæ°›å›´æ„Ÿã€‚

ã€åŸæ–‡ç‰‡æ®µã€‘
{context_text}

ã€ç”¨æˆ·é¢å¤–è¦æ±‚ã€‘
{extra_req}

è¯·ç›´æ¥è¾“å‡ºç»“æœã€‚
"""
                    
                    # ç¬¬ä¸€é˜¶æ®µè°ƒç”¨
                    first_stage_response = ""
                    with st.status("æ­£åœ¨è¿›è¡Œæ·±åº¦ç”Ÿæˆ...", expanded=True) as status:
                        st.write("ğŸ“ æ­£åœ¨ç”Ÿæˆåˆå§‹è§’è‰²è®¾å®šä¸å¯¹è¯...")
                        messages_gen = [{"role": "user", "content": gen_prompt}]
                        stream_gen = st.session_state.llm_client.chat(messages_gen, model=selected_model, stream=True)
                        
                        gen_placeholder = st.empty()
                        if isinstance(stream_gen, str):
                            st.error(stream_gen)
                            st.stop()
                        
                        for chunk in stream_gen:
                            if chunk.choices[0].delta.content:
                                content = chunk.choices[0].delta.content
                                first_stage_response += content
                                gen_placeholder.markdown(first_stage_response + "â–Œ")
                        gen_placeholder.markdown(first_stage_response)
                        
                        # 3. æ„å»º Prompt (ç¬¬äºŒé˜¶æ®µï¼šåˆ¤åˆ«ä¸ä¿®æ­£)
                        st.write("âš–ï¸ æ­£åœ¨è¿›è¡Œå‰§æƒ…é€»è¾‘ä¸äººè®¾æ ¡éªŒ...")
                        judge_prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‰§æƒ…é€»è¾‘å®¡æ ¸å‘˜ã€‚è¯·è¯„ä¼°ä»¥ä¸‹ç”Ÿæˆçš„è§’è‰²Promptå’Œå¯¹è¯æ˜¯å¦ç¬¦åˆåŸæ–‡çš„å‰§æƒ…é€»è¾‘å’Œäººè®¾ã€‚

ã€åŸæ–‡ç‰‡æ®µã€‘
{context_text}

ã€å¾…è¯„ä¼°ç”Ÿæˆçš„è®¾å®šã€‘
{first_stage_response}

ã€å®¡æ ¸è¦æ±‚ã€‘
1. **åˆ¤æ–­æ ‡å‡†**ï¼šé‡ç‚¹åˆ¤æ–­æ˜¯å¦ç¬¦åˆâ€œå‰§æƒ…é€»è¾‘â€å’Œâ€œäººè®¾è¿˜åŸåº¦â€ã€‚**å‰Šå¼±é€»è¾‘åˆ¤æ–­**ï¼Œä¸è¦è¿‡åˆ†çº ç»“ä¸¥å¯†çš„ç°å®é€»è¾‘ï¼Œåªè¦ç¬¦åˆæ•…äº‹å†…éƒ¨çš„å‰§æƒ…é€»è¾‘å³å¯ã€‚
2. **è¾“å‡ºå¤„ç†**ï¼š
   - å¦‚æœå†…å®¹åˆæ ¼ï¼Œè¯·ç›´æ¥è¾“å‡ºåŸå†…å®¹ã€‚
   - å¦‚æœæœ‰åå·®ï¼ˆå¦‚OOCã€è¯­æ°”ä¸å¯¹ã€å‰§æƒ…å†²çªï¼‰ï¼Œè¯·ä¿®æ­£å¹¶è¾“å‡ºä¼˜åŒ–åçš„å®Œæ•´ç‰ˆæœ¬ã€‚
3. **ä¿ç•™é¡¹**ï¼šç¡®ä¿è¾“å‡ºçš„æœ€åä¾ç„¶åŒ…å«â€œã€æå–çš„åŸæ–‡æœ¬è¡Œæ–‡é£æ ¼ã€‘â€ã€‚

è¯·è¾“å‡ºæœ€ç»ˆç¡®å®šçš„ç‰ˆæœ¬ã€‚
"""
                        messages_judge = [{"role": "user", "content": judge_prompt}]
                        stream_judge = st.session_state.llm_client.chat(messages_judge, model=selected_model, stream=True)
                        
                        final_response = ""
                        # Clear previous placeholder to show final result cleanly
                        gen_placeholder.empty() 
                        final_placeholder = st.empty()
                        
                        if isinstance(stream_judge, str):
                            st.error(stream_judge)
                        else:
                            for chunk in stream_judge:
                                if chunk.choices[0].delta.content:
                                    content = chunk.choices[0].delta.content
                                    final_response += content
                                    final_placeholder.markdown(final_response + "â–Œ")
                            final_placeholder.markdown(final_response)
                        
                        status.update(label="ç”Ÿæˆå®Œæˆ", state="complete", expanded=False)
                        
                        # é‡ç½®å¯¹è¯å†å²ï¼Œå­˜å…¥æœ€ç»ˆç»“æœ
                        st.session_state.gen_messages = [
                            {"role": "user", "content": gen_prompt}, # ä¿å­˜åˆå§‹è¯·æ±‚
                            {"role": "assistant", "content": final_response}
                        ]
                        st.rerun()

        # æ˜¾ç¤ºç”Ÿæˆå†å²å’Œå¯¹è¯
        for msg in st.session_state.gen_messages:
            if msg["role"] == "system":
                continue # ä¸æ˜¾ç¤ºç³»ç»ŸæŒ‡ä»¤
            if msg["role"] == "user":
                # éšè—åˆå§‹çš„å¤§æ®µ Promptï¼Œåªæ˜¾ç¤ºåç»­çš„ä¿®æ”¹æ„è§
                if msg["content"].startswith("ç›®æ ‡è§’è‰²ï¼š"):
                    with st.expander("æŸ¥çœ‹åˆå§‹ Prompt è¯·æ±‚"):
                        st.text(msg["content"])
                else:
                    with st.chat_message("user"):
                        st.markdown(msg["content"])
            else:
                with st.chat_message("assistant"):
                    st.markdown(msg["content"])

        # ä¿å­˜æŒ‰é’®
        if st.session_state.gen_messages and st.session_state.gen_messages[-1]["role"] == "assistant":
            if st.button("ğŸ’¾ ä¿å­˜å½“å‰ Prompt åˆ°å†å²è®°å½•"):
                last_response = st.session_state.gen_messages[-1]["content"]
                save_history_item(char_name, last_response)
                st.success("å·²ä¿å­˜ï¼")

        # ä¿®æ”¹æ„è§è¾“å…¥æ¡†
        if prompt := st.chat_input("å¯¹ç»“æœä¸æ»¡æ„ï¼Ÿè¯·è¾“å…¥ä¿®æ”¹æ„è§...", key="gen_chat"):
            if not st.session_state.gen_messages:
                st.warning("è¯·å…ˆç”Ÿæˆè§’è‰²æç¤ºè¯")
            else:
                st.session_state.gen_messages.append({"role": "user", "content": prompt})
                with st.chat_message("user"):
                    st.markdown(prompt)

                with st.chat_message("assistant"):
                    stream = st.session_state.llm_client.chat(st.session_state.gen_messages, model=selected_model, stream=True)
                    if isinstance(stream, str):
                        st.error(stream)
                    else:
                        full_response = ""
                        response_placeholder = st.empty()
                        for chunk in stream:
                            if chunk.choices[0].delta.content:
                                content = chunk.choices[0].delta.content
                                full_response += content
                                response_placeholder.markdown(full_response)
                        st.session_state.gen_messages.append({"role": "assistant", "content": full_response})

    # Tab 2: è‡ªç”±å¯¹è¯
    with tab2:
        st.markdown("### ä¸æ¨¡å‹å¯¹è¯ (å¯é€‰ RAG)")
        enable_rag = st.checkbox("å¯ç”¨ RAG (å¼•ç”¨çŸ¥è¯†åº“)", value=True, disabled=not st.session_state.vector_db_ready)
        
        # æ˜¾ç¤ºå†å²æ¶ˆæ¯
        for msg in st.session_state.messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        if prompt := st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜..."):
            # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
            st.session_state.messages.append({"role": "user", "content": prompt})
            with st.chat_message("user"):
                st.markdown(prompt)

            # ç”Ÿæˆå›å¤
            with st.chat_message("assistant"):
                if not st.session_state.llm_client:
                    st.error("è¯·å…ˆé…ç½® API Key")
                else:
                    context_str = ""
                    if enable_rag and st.session_state.vector_db_ready:
                        with st.spinner("æ£€ç´¢ä¸­..."):
                            docs = st.session_state.rag_engine.query(prompt, k=3)
                            context_str = "\n\n".join([doc.page_content for doc in docs])
                            with st.expander("å‚è€ƒä¸Šä¸‹æ–‡"):
                                st.text(context_str)
                    
                    # æ„å»ºæ¶ˆæ¯
                    messages_payload = []
                    # å¦‚æœæœ‰ RAG ä¸Šä¸‹æ–‡ï¼Œæ’å…¥åˆ° System Prompt æˆ– User Prompt ä¸­
                    if context_str:
                        system_msg = f"ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚è¯·åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚\n\nä¸Šä¸‹æ–‡ï¼š\n{context_str}"
                        messages_payload.append({"role": "system", "content": system_msg})
                    
                    # æ·»åŠ å†å²è®°å½• (ç®€å•å¤„ç†ï¼Œåªå–æœ€è¿‘å‡ è½®ä»¥èŠ‚çœ token)
                    for m in st.session_state.messages[-5:]:
                        messages_payload.append(m)
                    
                    # å¦‚æœæ²¡æœ‰ RAG ä¸”æ²¡æœ‰å†å² system promptï¼Œå¯ä»¥åŠ ä¸€ä¸ªé»˜è®¤çš„
                    if not context_str and not any(m['role'] == 'system' for m in messages_payload):
                         messages_payload.insert(0, {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚"})

                    # è°ƒç”¨ LLM
                    response_placeholder = st.empty()
                    full_response = ""
                    stream = st.session_state.llm_client.chat(messages_payload, model=selected_model, stream=True)
                    
                    if isinstance(stream, str):
                        st.error(stream)
                    else:
                        for chunk in stream:
                            if chunk.choices[0].delta.content:
                                content = chunk.choices[0].delta.content
                                full_response += content
                                response_placeholder.markdown(full_response)
                        
                        st.session_state.messages.append({"role": "assistant", "content": full_response})

    # Tab 3: QQè§’è‰²ç”Ÿæˆ
    with tab3:
        st.markdown("### ğŸ¤– QQèŠå¤©è§’è‰²Promptç”Ÿæˆ")
        st.markdown("é€šè¿‡ä¸AIå¯¹è¯æ¥åˆ›å»ºé€‚åˆQQèŠå¤©çš„è§’è‰²è®¾å®š")

        # ç¬¬ä¸€æ­¥ï¼šå¯¹è¯æ”¶é›†
        st.subheader("ğŸ“ ç¬¬ä¸€æ­¥ï¼šä¸AIè‡ªç”±å¯¹è¯")
        st.markdown("ä¸AIè¿›è¡Œè‡ªç”±å¯¹è¯ï¼Œå¸®åŠ©AIäº†è§£ä½ æƒ³è¦çš„è§’è‰²ç‰¹ç‚¹")

        # æ˜¾ç¤ºå¯¹è¯å†å²
        for msg in st.session_state.qq_dialogue_messages:
            with st.chat_message(msg["role"]):
                st.markdown(msg["content"])

        # å¯¹è¯è¾“å…¥
        if qq_prompt := st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜æˆ–å¯¹è¯å†…å®¹...", key="qq_dialogue"):
            if not st.session_state.llm_client:
                st.error("è¯·å…ˆé…ç½®API Key")
            else:
                # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
                st.session_state.qq_dialogue_messages.append({"role": "user", "content": qq_prompt})
                with st.chat_message("user"):
                    st.markdown(qq_prompt)

                # ç”ŸæˆAIå›å¤
                with st.chat_message("assistant"):
                    response_placeholder = st.empty()
                    full_response = ""

                    # æ„å»ºæ¶ˆæ¯
                    messages_payload = [{"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ï¼Œè¯·ä¸ç”¨æˆ·è¿›è¡Œè‡ªç„¶ã€æµç•…çš„å¯¹è¯ã€‚é€šè¿‡å¯¹è¯äº†è§£ç”¨æˆ·çš„å–œå¥½ã€æ€§æ ¼ç‰¹ç‚¹ï¼Œä¸ºåç»­ç”ŸæˆQQèŠå¤©è§’è‰²è®¾å®šåšå‡†å¤‡ã€‚"}]
                    for m in st.session_state.qq_dialogue_messages[-10:]:  # åªä¿ç•™æœ€è¿‘10è½®å¯¹è¯
                        messages_payload.append(m)

                    stream = st.session_state.llm_client.chat(messages_payload, model=selected_model, stream=True)

                    if isinstance(stream, str):
                        st.error(stream)
                    else:
                        for chunk in stream:
                            if chunk.choices[0].delta.content:
                                content = chunk.choices[0].delta.content
                                full_response += content
                                response_placeholder.markdown(full_response + "â–Œ")

                        response_placeholder.markdown(full_response)
                        st.session_state.qq_dialogue_messages.append({"role": "assistant", "content": full_response})

        # ç¬¬äºŒæ­¥ï¼šç”ŸæˆQQè§’è‰²Prompt
        st.divider()
        st.subheader("ğŸ¯ ç¬¬äºŒæ­¥ï¼šç”ŸæˆQQèŠå¤©Prompt")

        col_gen1, col_gen2 = st.columns([1, 1])
        with col_gen1:
            if st.button("ğŸ“ ç”Ÿæˆè§’è‰²Prompt", disabled=not st.session_state.llm_client or not st.session_state.qq_dialogue_messages):
                if not st.session_state.qq_dialogue_messages:
                    st.warning("è¯·å…ˆè¿›è¡Œä¸€äº›å¯¹è¯æ¥å¸®åŠ©AIäº†è§£è§’è‰²ç‰¹ç‚¹")
                else:
                    with st.spinner("æ­£åœ¨åˆ†æå¯¹è¯å¹¶ç”Ÿæˆè§’è‰²è®¾å®š..."):
                        # æ„å»ºç”Ÿæˆprompt
                        dialogue_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in st.session_state.qq_dialogue_messages])

                        gen_prompt = f"""åŸºäºä»¥ä¸‹å¯¹è¯å†…å®¹ï¼Œè¯·ä¸ºQQèŠå¤©ç”Ÿæˆä¸€ä¸ªè§’è‰²Promptã€‚

ã€å¯¹è¯è®°å½•ã€‘
{dialogue_text}

è¯·ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„QQèŠå¤©è§’è‰²Promptï¼ŒåŒ…å«ä»¥ä¸‹éƒ¨åˆ†ï¼š

1. ã€äººè®¾åŸºæœ¬ä¿¡æ¯ã€‘ï¼šè§’è‰²çš„å§“åã€å¹´é¾„ã€æ€§åˆ«ã€èŒä¸šç­‰åŸºæœ¬ä¿¡æ¯
2. ã€äººç‰©èƒŒæ™¯ã€‘ï¼šè§’è‰²çš„èº«ä¸–èƒŒæ™¯ã€ç»å†ã€æ€§æ ¼ç‰¹ç‚¹ç­‰
3. ã€èŠå¤©å¯¹è¯è¦æ±‚ã€‘ï¼šè§’è‰²çš„è¯´è¯é£æ ¼ã€è¯­æ°”ã€å¸¸ç”¨è¡¨æƒ…ã€èŠå¤©ä¹ æƒ¯ç­‰
4. ã€å¯¹è¯ç¤ºä¾‹ã€‘ï¼šè¯·æä¾›5ä¸ªå…·ä½“çš„å¯¹è¯ç¤ºä¾‹ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
   ç”¨æˆ·ï¼šæ¶ˆæ¯å†…å®¹
   è§’è‰²ï¼šå›å¤å†…å®¹

è¯·ç¡®ä¿å¯¹è¯ç¤ºä¾‹è´´åˆè§’è‰²çš„æ€§æ ¼å’Œè¯´è¯é£æ ¼ã€‚
"""

                        messages_gen = [{"role": "user", "content": gen_prompt}]
                        response = st.session_state.llm_client.chat(messages_gen, model=selected_model, stream=False)

                        if isinstance(response, str):
                            st.error(response)
                        else:
                            full_response = response.choices[0].message.content

                            # è§£æç”Ÿæˆçš„prompt
                            try:
                                # ç®€å•çš„æ–‡æœ¬è§£æ
                                lines = full_response.split('\n')
                                character_info = ""
                                background = ""
                                chat_requirements = ""
                                examples = []

                                current_section = ""
                                example_lines = []

                                for line in lines:
                                    line = line.strip()
                                    if line.startswith("ã€äººè®¾åŸºæœ¬ä¿¡æ¯ã€‘"):
                                        current_section = "character_info"
                                        character_info = line.replace("ã€äººè®¾åŸºæœ¬ä¿¡æ¯ã€‘", "").strip()
                                    elif line.startswith("ã€äººç‰©èƒŒæ™¯ã€‘"):
                                        current_section = "background"
                                        background = line.replace("ã€äººç‰©èƒŒæ™¯ã€‘", "").strip()
                                    elif line.startswith("ã€èŠå¤©å¯¹è¯è¦æ±‚ã€‘"):
                                        current_section = "chat_requirements"
                                        chat_requirements = line.replace("ã€èŠå¤©å¯¹è¯è¦æ±‚ã€‘", "").strip()
                                    elif line.startswith("ã€å¯¹è¯ç¤ºä¾‹ã€‘"):
                                        current_section = "examples"
                                    elif current_section == "character_info" and not line.startswith("ã€"):
                                        character_info += "\n" + line
                                    elif current_section == "background" and not line.startswith("ã€"):
                                        background += "\n" + line
                                    elif current_section == "chat_requirements" and not line.startswith("ã€"):
                                        chat_requirements += "\n" + line
                                    elif current_section == "examples":
                                        if "ï¼š" in line:
                                            parts = line.split("ï¼š", 1)
                                            if len(parts) == 2:
                                                role = parts[0].strip()
                                                content = parts[1].strip()
                                                if role == "ç”¨æˆ·" or role == "è§’è‰²":
                                                    example_lines.append({"role": "user" if role == "ç”¨æˆ·" else "character", "content": content})
                                                if len(example_lines) >= 2:
                                                    examples.append({
                                                        "user": example_lines[-2]["content"] if example_lines[-2]["role"] == "user" else "",
                                                        "character": example_lines[-1]["content"] if example_lines[-1]["role"] == "character" else ""
                                                    })
                                                    example_lines = []

                                # æ›´æ–°session state
                                st.session_state.qq_prompt_data = {
                                    "character_info": character_info.strip(),
                                    "background": background.strip(),
                                    "chat_requirements": chat_requirements.strip(),
                                    "dialogue_examples": examples[:5] if examples else st.session_state.qq_prompt_data["dialogue_examples"]
                                }

                                st.success("è§’è‰²Promptç”Ÿæˆå®Œæˆï¼")
                                st.rerun()

                            except Exception as e:
                                st.error(f"è§£æç”Ÿæˆç»“æœå¤±è´¥: {e}")
                                st.text_area("ç”Ÿæˆçš„å®Œæ•´å†…å®¹", full_response, height=300)

        with col_gen2:
            if st.button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", disabled=not st.session_state.qq_dialogue_messages):
                st.session_state.qq_dialogue_messages = []
                st.success("å¯¹è¯å·²æ¸…ç©º")
                st.rerun()

        # ç¬¬ä¸‰æ­¥ï¼šç¼–è¾‘å’Œè°ƒæ•´
        if st.session_state.qq_prompt_data["character_info"] or st.session_state.qq_prompt_data["background"]:
            st.divider()
            st.subheader("âœï¸ ç¬¬ä¸‰æ­¥ï¼šç¼–è¾‘å’Œè°ƒæ•´")

            # æ˜¾ç¤ºå½“å‰Prompt
            with st.expander("ğŸ“‹ å½“å‰è§’è‰²Prompt", expanded=True):
                st.markdown("**äººè®¾åŸºæœ¬ä¿¡æ¯ï¼š**")
                st.text_area("äººè®¾åŸºæœ¬ä¿¡æ¯", st.session_state.qq_prompt_data["character_info"], height=100, key="edit_character_info", on_change=lambda: update_qq_prompt_data())

                st.markdown("**äººç‰©èƒŒæ™¯ï¼š**")
                st.text_area("äººç‰©èƒŒæ™¯", st.session_state.qq_prompt_data["background"], height=150, key="edit_background", on_change=lambda: update_qq_prompt_data())

                st.markdown("**èŠå¤©å¯¹è¯è¦æ±‚ï¼š**")
                st.text_area("èŠå¤©å¯¹è¯è¦æ±‚", st.session_state.qq_prompt_data["chat_requirements"], height=150, key="edit_chat_requirements", on_change=lambda: update_qq_prompt_data())

                st.markdown("**å¯¹è¯ç¤ºä¾‹ï¼š**")
                for i, example in enumerate(st.session_state.qq_prompt_data["dialogue_examples"]):
                    col_e1, col_e2 = st.columns(2)
                    with col_e1:
                        st.text_input(f"ç”¨æˆ·æ¶ˆæ¯ {i+1}", example["user"], key=f"user_msg_{i}", on_change=lambda idx=i: update_example(idx, "user"))
                    with col_e2:
                        st.text_input(f"è§’è‰²å›å¤ {i+1}", example["character"], key=f"char_msg_{i}", on_change=lambda idx=i: update_example(idx, "character"))

            # è°ƒæ•´æŒ‰é’®
            if st.button("ğŸ”„ æ ¹æ®ç¤ºä¾‹è°ƒæ•´å¯¹è¯è¦æ±‚", disabled=not st.session_state.llm_client):
                with st.spinner("æ­£åœ¨æ ¹æ®å¯¹è¯ç¤ºä¾‹è°ƒæ•´å¯¹è¯è¦æ±‚..."):
                    examples_text = "\n".join([f"ç”¨æˆ·ï¼š{ex['user']}\nè§’è‰²ï¼š{ex['character']}" for ex in st.session_state.qq_prompt_data["dialogue_examples"]])

                    adjust_prompt = f"""åŸºäºä»¥ä¸‹å¯¹è¯ç¤ºä¾‹ï¼Œè¯·ä¼˜åŒ–èŠå¤©å¯¹è¯è¦æ±‚éƒ¨åˆ†ï¼š

ã€å½“å‰å¯¹è¯è¦æ±‚ã€‘
{st.session_state.qq_prompt_data["chat_requirements"]}

ã€å¯¹è¯ç¤ºä¾‹ã€‘
{examples_text}

è¯·æ ¹æ®è¿™äº›å¯¹è¯ç¤ºä¾‹ï¼Œé‡æ–°ä¼˜åŒ–"èŠå¤©å¯¹è¯è¦æ±‚"éƒ¨åˆ†ï¼Œä½¿å…¶æ›´å‡†ç¡®åœ°åæ˜ è§’è‰²çš„è¯´è¯é£æ ¼ã€è¯­æ°”å’ŒèŠå¤©ä¹ æƒ¯ã€‚
åªè¾“å‡ºä¼˜åŒ–åçš„å¯¹è¯è¦æ±‚å†…å®¹ï¼Œä¸è¦åŒ…å«å…¶ä»–è¯´æ˜ã€‚
"""

                    messages_adjust = [{"role": "user", "content": adjust_prompt}]
                    response = st.session_state.llm_client.chat(messages_adjust, model=selected_model, stream=False)

                    if isinstance(response, str):
                        st.error(response)
                    else:
                        new_requirements = response.choices[0].message.content.strip()
                        st.session_state.qq_prompt_data["chat_requirements"] = new_requirements
                        st.success("å¯¹è¯è¦æ±‚å·²æ›´æ–°ï¼")
                        st.rerun()

            # ä¿å­˜æŒ‰é’®
            if st.button("ğŸ’¾ ä¿å­˜åˆ°å†å²è®°å½•"):
                prompt_content = f"""ã€äººè®¾åŸºæœ¬ä¿¡æ¯ã€‘
{st.session_state.qq_prompt_data["character_info"]}

ã€äººç‰©èƒŒæ™¯ã€‘
{st.session_state.qq_prompt_data["background"]}

ã€èŠå¤©å¯¹è¯è¦æ±‚ã€‘
{st.session_state.qq_prompt_data["chat_requirements"]}

ã€å¯¹è¯ç¤ºä¾‹ã€‘
""" + "\n".join([f"ç”¨æˆ·ï¼š{ex['user']}\nè§’è‰²ï¼š{ex['character']}\n" for ex in st.session_state.qq_prompt_data["dialogue_examples"]])

                char_name = "QQè§’è‰²"
                if st.session_state.qq_prompt_data["character_info"]:
                    # å°è¯•æå–è§’è‰²åç§°
                    first_line = st.session_state.qq_prompt_data["character_info"].split('\n')[0]
                    if "ï¼š" in first_line:
                        char_name = first_line.split("ï¼š")[1].strip()

                save_history_item(char_name, prompt_content)
                st.success("å·²ä¿å­˜åˆ°å†å²è®°å½•ï¼")

    # Tab 4: å†å²è®°å½•
    with tab3:
        st.markdown("### ğŸ“œ å†å² Prompt è®°å½•")
        history = load_history()
        if not history:
            st.info("æš‚æ— å†å²è®°å½•ã€‚")
        else:
            for i, item in enumerate(history):
                with st.expander(f"{item['timestamp']} - {item['char_name']}"):
                    st.code(item['content'], language="markdown")
                    if st.button("åˆ é™¤", key=f"del_{i}"):
                        delete_history_item(i)
                        st.rerun()

if __name__ == "__main__":
    main()
